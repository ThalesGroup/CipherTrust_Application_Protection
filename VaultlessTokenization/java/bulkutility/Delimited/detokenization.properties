###############################################################################
# VaultLess Tokenization Bulk Migration Configuration
#
# To Detokenize use the following command
#
#   java com.safenet.vaultLessTokenization.migration.main config-file-path -d
#
# Note: This is a sample file and needs to be customized to your specific
#       environment
#
###############################################################################

#####################
# Input Configuration
#   Input.FilePath
#   Input.Type
#####################
#
# Input.FilePath
#
# Full path to the input file
#

Input.FilePath = C:\\test\\Delimited\\tokenized.csv
#
# Input.Type
#
# Format of the input file
#
# Valid values
#   Delimited
#   Positional
#
Input.Type = Delimited

###############################
# Delimited Input Configuration
#   Input.EscapeCharacter
#   Input.QuoteCharacter
#   Input.ColumnDelimiter
###############################
#
# Input.EscapeCharacter
#
# Specifies a character that is used to 'escape' special characters that
# alter input processing
#
# Note: this parameter is ignored if Input.Type is set to Positional
#
Input.EscapeCharacter = \\

#
# Input.QuoteCharacter
#
# Specifies a character that is used around character sequences that contain
# delimiter characters and are to be treated as a single column value
#
# Note: this parameter is ignored if Input.Type is set to Positional
# 
Input.QuoteCharacter = "

#
# Input.ColumnDelimiter
#
# Specifies a character that separates columns in the input file
#
# Note: this parameter is ignored if Input.Type is set to Positional
#
Input.ColumnDelimiter = ,

################################
# Positional Input Configuration
#   Input.Column0.Start
#   Input.Column0.End
#   ...
#   Input.ColumnN.Start
#   Input.ColumnN.End
################################

# 
# Input.ColumnN.Start
#
# Specifies zero-based position where the value starts. The character in the
# specified position is included in the column value. This value must be
# specified for every column in the input file which has to be processed
# or passed-through and included in the output file.
#
# Note: this parameter is ignored if Input.Type is set to Delimited
#
Input.Column0.Start = 

# 
# Input.ColumnN.End
#
# Specifies zero-based position where the value ends. The character in the
# specified position is included in the column value. This value must be
# specified for every column in the input file which has to be processed
# or passed-through and included in the output file.
#
# Note: this parameter is ignored if Input.Type is set to Delimited
#
Input.Column0.End = 

###########################################
# AlgoSpec Configuration
#	Version
#	Unicode
############################################

#
# AlgoSpec.Version
# Specifies the version for AlgoSpec
#
# Valid values : 0 or 1
#
# Recommended : 1
#
AlgoSpec.Version = 1

# AlgoSpec.Unicode
# Specifies the Unicode Block to be used for Tokenization

AlgoSpec.Unicode =

###########################################
# TokenSpec Configuration
#   DeTokenizer.Column0.GroupId
#   DeTokenizer.Column0.TokenFormat
#   DeTokenizer.Column0.ClearTextSensitive
#   DeTokenizer.Column0.LuhnCheck
#   DeTokenizer.Column0.NonIdempotentTokens
#   ...
#   DeTokenizer.ColumnN.GroupId
#   DeTokenizer.ColumnN.TokenFormat
#   DeTokenizer.ColumnN.ClearTextSensitive
#   DeTokenizer.ColumnN.LuhnCheck
#   DeTokenizer.ColumnN.NonIdempotentTokens
############################################

# DeTokenizer.ColumnN.GroupId
#
# Specifies the GroupId for this column.
# Default value : 0
#
DeTokenizer.Column5.GroupId	= 0

#
# DeTokenizer.ColumnN.Token.TokenFormat
#
# Specifies token format that will be used to detokenize this column.
# It is required to provide value for this parameter. 
# If not specified, then no detokenization will be performed for this column.
#
DeTokenizer.Column5.TokenFormat	= LAST_FOUR_TOKEN

#
# DeTokenizer.ColumnN.ClearTextSensitive
#
# Specifies whether the token being detokenized is clearTextSensitive or not.
# It is valid only if token format is other than TOKEN_ALL.
# Default value : false
# Recommended : true
#
DeTokenizer.Column5.ClearTextSensitive = true

#
# DeTokenizer.ColumnN.LuhnCheck
#
# Specifies whether the token being detokenized will pass or fail luhn check. 
# If no value is provided, then luhn check for the token will be ignored.
# If provided, make sure to provide input which is luhn compliant.
#
# Valid values
#   true
#   false
#
DeTokenizer.Column5.LuhnCheck = 

#
# DeTokenizer.ColumnN.NonIdempotentTokens
#
# Specifies if token being detokenized is NonIdempotent token or not.
# It is applicable only when LuhnCheck is set to false.
#
DeTokenizer.Column5.NonIdempotentTokens	= 

######################
# Output Configuration
#   Output.FilePath
#   Output.Sequence
######################

#
# Output.FilePath
#
# Specifies full path to the output file
#
Output.FilePath = C:\\test\\Delimited\\out.csv

#
#Intermediate.FilePath
#
# Specifies the intermediate file path in which the intermediate temporary chunks will be stored
# 
# If not specified, then the Output.FilePath will be used as Intermediate.FilePath
#
Intermediate.FilePath =

#
# Output.Sequence
#
# Specifies sequence of the input columns in which they are written to the
# output file. Each column in the input file that has to appear in the
# output file has to have its column index specified in the output sequence.
# For each column in the input file, the sequence number can be either positive
# or negative. Positive sequence number indicates that the decrypted and/or
# tokenized value is written to the output file, if the column was decrypted
# and/or tokenized. Negative sequence number indicates that the original value
# from the input file is written to the output file. For columns that are not
# decrypted and not tokenized (pass-through columns) specifying positive or
# negative number has the same effect.
# Column indexes are separated by , character.
#
Output.Sequence = 0,-1,-2,-3,-4,5,-6

###############################
# Multi-threading Configuration
#   Threads.BatchSize
#   Threads.CryptoThreads
#   Threads.TokenThreads
#   Threads.PollTimeout
###############################

#
# Threads.BatchSize
#
# Specifies number of rows per batch.
#
Threads.BatchSize = 10000

#
# Threads.FormatterThreads
#
# Specifies number of threads that will format of columns from input file
# as required.
#
Threads.FormatterThreads = 20

#
# Threads.DetokenThreads
#
# Specifies number of threads that will perform detokenization of columns
# as required.
#
Threads.DetokenThreads = 20

#
# Threads.PollTimeout
#
# Specifies the amount of time (in milliseconds) processing threads will
# wait for a batch on the data queue before timing out, checking for 
# adminitrative commands on the management queue, and then checking for
# another batch on the data queue.
# Default value of this parameter is 100.
# Do not modify this parameter unless instructed by customer support.
#
Threads.PollTimeout = 100

#
# Logger.LogLevel
#
# Specifies the level of details displayed
#
# Valid values
#   Normal
#   Verbose
#
Logger.LogLevel = Normal

#
# Obfucate password
#
# Specifies if the provided passwords are obfuscated or not
#
# Valid values
#   true
#   false
#
PasswordObfuscation = false

#
#TokenSeparator
#
# Specifies if the output values are space separated or not.
#
# Note: this parameter is ignored if Input.Type is set to Delimited.
#
# Valid values
#   true
#   false
#
# Default Value - true


TokenSeparator = true

#
# StreamInputData
#
# Specifies whether the input data is streamed or not.
#
# Note: this parameter is used only if Input.Type is set to Positional.
#
# Valid values
#   true
#   false
#
# Default Value - false
#
StreamInputData = false

#
# CodePageUsed
#
# Specifies the code page in use .
#
# Used with EBCDIC character set for ex. use "ibm500" for EBCDIC International
# https://docs.oracle.com/javase/7/docs/api/java/nio/charset/Charset.html

CodePageUsed =  

###############################################################################
# END
###############################################################################
